{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAYPwLK37icX"
      },
      "source": [
        "# NLP 2 과제\n",
        "> 인공지능 스터디 일곱 번째 과제에 오신 것을 환영합니다! 강의를 들으면서 배운 다양한 지식들을 실습을 통해서 활용해 볼 시간을 가질 것입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDFP_3m7ica"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
        "\n",
        "아래의 수식과 같이 계산되는 multi-head attention에서 query, key, value 벡터를 생성하기 위한\n",
        "projection matrix $( W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}​​ )$는 head 간에 sharing 된다. <br>\n",
        "***\n",
        "$ MultiHead(Q,K,V)=Concat(head_1, \\cdots, head_h)W^{O} $ (이때, $W^{O}$ 는 Output을 만들때 사용되는 가중치 행렬)<br>\n",
        "where $head_i=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$ (이때, $Q, K, V$ 는 입력에서 tokenize된 단어들의 임베딩 벡터 $Q = K = V$ )\n",
        "```python\n",
        "(1) 예\n",
        "(2) 아니오\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG6Ti5SW7icb"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmP7gTWF7icb"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
        "```python\n",
        "Transformer 모델에서 각 입력 토큰들이 가진 순서를 입력하기 위해 사용하는 방법을 고르시오.\n",
        "\n",
        "(1) Positional Encoding\n",
        "(2) Encoder-Decoder attention\n",
        "(3) Layer normalization\n",
        "(4) Masked decoder self-attention\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5_XGnos7icc"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-1a8qNT7icc"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 모델이 어떻게 다양한 자연어 처리 태스크에서 사용될 수 있는지 설명해주세요.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4BU3MK7icc"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "논리적 함의 추측, 유사성 검사, 다지선다문제 정답 선택\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DUSVATt7icc"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 모델의 \"GPT\" 약자는 무엇을 의미하나요?\n",
        "\n",
        "(1) Generalized Pre-trained Transformer\n",
        "(2) Generative Pre-trained Transformer\n",
        "(3) Globalized Pre-processing Transformer\n",
        "(4) Gradient Propagation Technique\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7itJhcZ7icc"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkRGcFB7icd"
      },
      "source": [
        "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> BERT\n",
        "```python\n",
        "다음 중 BERT에 대한 설명으로 옳지 않은 것을 고르시오.\n",
        "\n",
        "(1) 학습 데이터에서 [MASK] 토큰이 선택되는 비율이 극단적으로 작은 경우, 모델 학습을 위한 비용이 증가한다.\n",
        "(2) Unidirectional model로 자연어 생성에 특화된 모델이다.\n",
        "(3) 입력 시퀀스 중 일부 마스킹된 토큰을 맞추는 masked language modeling (masked LM)을 통해 pre-training을 수행하였다.\n",
        "(4) 사전학습을 위한 [MASK] 토큰은 random하게 선택된다.\n",
        "(5) Unlabeled 데이터를 기반으로 self-supervised learning을 적용하여 사전학습한 모델이다.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0-9QdQa7icd"
      },
      "source": [
        "```python\n",
        "😉\n",
        "# TODO : 정답을 적어주세요\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQrt9UOF7icd"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 실습 ]</b></font> Multi-head Attention\n",
        "```python\n",
        "이번 실습을 통해 다음 2가지를 알아볼 것입니다.\n",
        "1. Multi-head attention 및 self-attention을 구현합니다.\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태를 이해합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJjr2VpE7ice"
      },
      "source": [
        "```python\n",
        "🐙\n",
        "먼저 코드 실행에 필요한 패키지를 import 해봅시다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lnnhjIo37ice"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52v_vrwh7icf"
      },
      "source": [
        "### 데이터 전처리\n",
        "```python\n",
        "저번 주차의 데이터와 비슷한 형태입니다.\n",
        "먼저 전체 단어 수인 vocab_size가 주어집니다.\n",
        "pad_id는 주어진 데이터의 길이를 맞춰주기 위해 패딩을 진행하게 되는데 이때 패딩을 의미하는 토큰의 id입니다.\n",
        "sample data 보면 숫자로 이루어진 것을 볼 수 있는데 이는 저희가 구성한 vocab에서 몇 번째 단어인지를 의미합니다.\n",
        "따라서 데이터의 각 요소를 단어로 이루어진 문장이라고 생각할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hgrXjEpJ7icg"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100\n",
        "pad_id = 0\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO396mcz7icg"
      },
      "source": [
        "```python\n",
        "주어진 데이터의 길이를 맞춰주기 위한 padding 함수를 도입합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MdQdZmXL7icg"
      },
      "outputs": [],
      "source": [
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzUALIKw7ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2446384-b00d-421a-fa75-3d50bc5de2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 62508.26it/s]\n"
          ]
        }
      ],
      "source": [
        "data, max_len = padding(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA7OYDMF7ici"
      },
      "source": [
        "```python\n",
        "전처리된 데이터를 확인해 보면 잘 패딩 되었음을 확인할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lZbTruS77ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa29a62e-62e3-446b-9973-ac09c6316190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGnTiEqm7icj"
      },
      "source": [
        "### Hyperparameter 세팅 및 embedding\n",
        "```python\n",
        "위 데이터를 임베딩하여 실습에 사용할 데이터를 만들어 봅시다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2Ktj2JXZ7icj"
      },
      "outputs": [],
      "source": [
        "d_model = 512  # model의 hidden size\n",
        "num_heads = 8  # multi-head에서의 head의 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JdtJv_mK7icj"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: 배치 사이즈, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lP8s6bSE7icj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e20d5f-decb-4aa8-ced8-1a7d81f1f29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4282,  0.6188,  0.8736,  ..., -0.8371, -0.5100,  0.7989],\n",
            "         [-1.9735,  0.2280,  0.0281,  ...,  2.0192,  0.1319,  0.0241],\n",
            "         [ 0.7195,  0.2482,  1.1721,  ...,  1.1802,  0.2993,  1.6275],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[-2.1553,  1.5210,  0.4982,  ..., -1.4450,  2.2981, -0.1059],\n",
            "         [-0.7475, -0.3579, -0.2764,  ...,  0.5888, -1.6453,  1.4642],\n",
            "         [ 0.3176, -1.8282,  1.1573,  ..., -0.2372, -1.3562, -0.9574],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[-1.3934, -0.3462, -0.8814,  ...,  1.1212, -1.7532,  0.0119],\n",
            "         [ 0.4660,  0.4771,  0.2235,  ...,  0.5900,  0.5305,  0.4062],\n",
            "         [-0.1280,  0.4496,  0.8122,  ...,  0.0750,  0.5611,  0.3438],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7640,  0.3421, -3.3202,  ...,  2.4083,  1.0090,  1.0383],\n",
            "         [-0.1898,  1.1985,  0.3842,  ...,  2.2184, -1.4027, -0.8913],\n",
            "         [ 0.3176, -1.8282,  1.1573,  ..., -0.2372, -1.3562, -0.9574],\n",
            "         ...,\n",
            "         [ 1.3513,  0.6911, -1.0318,  ...,  0.5124,  0.4218, -3.4490],\n",
            "         [ 1.4607,  0.3821,  0.8196,  ..., -0.0147, -1.6442,  0.0448],\n",
            "         [ 0.1682, -0.9391,  2.3732,  ...,  0.7258, -0.1115,  1.4364]],\n",
            "\n",
            "        [[-0.1201,  0.7537, -0.9623,  ...,  1.5528,  0.3273,  2.0615],\n",
            "         [-0.5071,  1.2079,  0.6430,  ...,  0.1678, -0.8953,  1.3479],\n",
            "         [ 0.8689, -1.1293, -0.4145,  ..., -1.0937,  0.5434, -0.4408],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[ 0.2676,  1.6211, -1.3688,  ..., -0.2610,  0.2634,  0.7333],\n",
            "         [-0.5071,  1.2079,  0.6430,  ...,  0.1678, -0.8953,  1.3479],\n",
            "         [ 1.1269, -1.8938,  1.1857,  ..., -0.0399, -0.2727, -2.2429],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgjjqmOP7icj"
      },
      "source": [
        "### Linear transformation & 여러 head로 나누기\n",
        "```python\n",
        "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다.\n",
        "\n",
        "query, key, value를 서로 다른 linear transformation matrix로 행렬 연산을 통해 만들어 냅니다. 따라서 동일한 데이터(batch_emb)로부터 서로 다른 query, key, value를 생성할 수 있습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9bZJNvK57icj"
      },
      "outputs": [],
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9RzeDr_7icj"
      },
      "source": [
        "```python\n",
        "output layer에서 사용될 행렬도 만들어 줍니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7b7BwyqA7ick"
      },
      "outputs": [],
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P_kCGno67ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc347e2-4b50-476a-8a15-81c79085b7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO2e0nJd7ick"
      },
      "source": [
        "```python\n",
        "q, k, v를 'num_head' 개의 차원으로 분할하여 여러 벡터를 만듭니다.\n",
        "실제 q, k, v 각각의 벡터 크기는 512가 아닌 64입니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LtSA5ltJ7ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea4014c-9bf6-4d58-e926-c9a6c826bc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads # q, k, v 벡터 사이즈\n",
        "\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QEGqCSE7ick"
      },
      "source": [
        "```python\n",
        "8개의 head에 필요한 q, k, v가 만들어졌습니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oijExEqd7ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e98ddcd-ac5c-46e8-ae8f-0cab7c236496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTefr-GC7ick"
      },
      "source": [
        "### Scaled dot-product self-attention 구현\n",
        "```python\n",
        "각 head에서 실행되는 self-attention 과정을 살펴봅시다.\n",
        "\n",
        "q, k 벡터의 내적 연산 이후에 d_k의 제곱근으로 나눠줍니다.\n",
        "이는 q와 k를 구성하는 요소의 평균과 분산을 내적의 결괏값에 대해서도 유지시켜주기 위함입니다.\n",
        "\n",
        "이후 계산된 각 행에 대해서 softmax 연산을 통해서 각 요소의 합을 1로 만들어줍니다.\n",
        "```\n",
        "- [Scaled Dot-Product Attention 참고](https://paperswithcode.com/method/scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "71QTPtz27ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cb9fce-2ff6-42b6-97c5-1cdfe4ff588b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0330, 0.0564, 0.0388,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          [0.0461, 0.0785, 0.0470,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0649, 0.0714, 0.0281,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         [[0.0437, 0.0439, 0.0536,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0360, 0.0788, 0.0283,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          [0.0556, 0.0511, 0.0556,  ..., 0.0346, 0.0346, 0.0346],\n",
            "          ...,\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599]],\n",
            "\n",
            "         [[0.0294, 0.0393, 0.0628,  ..., 0.0620, 0.0620, 0.0620],\n",
            "          [0.0606, 0.0325, 0.0830,  ..., 0.0384, 0.0384, 0.0384],\n",
            "          [0.0884, 0.0367, 0.0656,  ..., 0.0298, 0.0298, 0.0298],\n",
            "          ...,\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0417, 0.0476, 0.0513,  ..., 0.0660, 0.0660, 0.0660],\n",
            "          [0.0614, 0.0197, 0.0377,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0847, 0.0524, 0.0574,  ..., 0.0288, 0.0288, 0.0288],\n",
            "          ...,\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563]],\n",
            "\n",
            "         [[0.0562, 0.0396, 0.0660,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0966, 0.0669, 0.0301,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0665, 0.0878, 0.0604,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          ...,\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632]],\n",
            "\n",
            "         [[0.0358, 0.0319, 0.0813,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          [0.0611, 0.0504, 0.0442,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0409, 0.0609, 0.0632,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          ...,\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408]]],\n",
            "\n",
            "\n",
            "        [[[0.0415, 0.0281, 0.0190,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0505, 0.0608, 0.0549,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0272, 0.0170, 0.0289,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          ...,\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524]],\n",
            "\n",
            "         [[0.0143, 0.0378, 0.0391,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0604, 0.0837, 0.0771,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0676, 0.1128, 0.0509,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          ...,\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520]],\n",
            "\n",
            "         [[0.0371, 0.0558, 0.0513,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0379, 0.0233, 0.0323,  ..., 0.0552, 0.0552, 0.0552],\n",
            "          [0.0408, 0.0253, 0.0634,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          ...,\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1056, 0.0736, 0.0573,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0468, 0.0961, 0.0293,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0290, 0.0894, 0.1124,  ..., 0.0457, 0.0457, 0.0457],\n",
            "          ...,\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504]],\n",
            "\n",
            "         [[0.0458, 0.0521, 0.0491,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0417, 0.0359, 0.0399,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0657, 0.0442, 0.0607,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          ...,\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515]],\n",
            "\n",
            "         [[0.0634, 0.0417, 0.0503,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0314, 0.0303, 0.0405,  ..., 0.0556, 0.0556, 0.0556],\n",
            "          [0.0757, 0.0511, 0.0386,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          ...,\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485]]],\n",
            "\n",
            "\n",
            "        [[[0.0592, 0.0739, 0.0699,  ..., 0.0326, 0.0326, 0.0326],\n",
            "          [0.0372, 0.0330, 0.0410,  ..., 0.0531, 0.0531, 0.0531],\n",
            "          [0.0353, 0.0840, 0.0262,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          ...,\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584]],\n",
            "\n",
            "         [[0.0548, 0.0345, 0.0281,  ..., 0.0587, 0.0587, 0.0587],\n",
            "          [0.0453, 0.0479, 0.0493,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0729, 0.0867, 0.0544,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          ...,\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512]],\n",
            "\n",
            "         [[0.0319, 0.0654, 0.0487,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0632, 0.0313, 0.0610,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0484, 0.0514, 0.0553,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          ...,\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0494, 0.0547, 0.0254,  ..., 0.0608, 0.0608, 0.0608],\n",
            "          [0.0885, 0.0538, 0.0580,  ..., 0.0411, 0.0411, 0.0411],\n",
            "          [0.0653, 0.0466, 0.0339,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528],\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528],\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528]],\n",
            "\n",
            "         [[0.0583, 0.0246, 0.0442,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          [0.0374, 0.0476, 0.0551,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          [0.0581, 0.0287, 0.0490,  ..., 0.0613, 0.0613, 0.0613],\n",
            "          ...,\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614]],\n",
            "\n",
            "         [[0.0311, 0.0397, 0.0577,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          [0.0430, 0.0879, 0.0702,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0386, 0.0507, 0.0432,  ..., 0.0399, 0.0399, 0.0399],\n",
            "          ...,\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0478, 0.0240, 0.0395,  ..., 0.0389, 0.0493, 0.0322],\n",
            "          [0.0697, 0.0500, 0.0320,  ..., 0.0430, 0.0426, 0.0462],\n",
            "          [0.1078, 0.0369, 0.0356,  ..., 0.0398, 0.0786, 0.0388],\n",
            "          ...,\n",
            "          [0.0785, 0.0629, 0.0371,  ..., 0.0374, 0.0293, 0.0498],\n",
            "          [0.0316, 0.0699, 0.0479,  ..., 0.0574, 0.0314, 0.0550],\n",
            "          [0.0690, 0.0518, 0.0455,  ..., 0.0363, 0.0475, 0.0501]],\n",
            "\n",
            "         [[0.0535, 0.0527, 0.0548,  ..., 0.0577, 0.0478, 0.0417],\n",
            "          [0.0372, 0.0363, 0.0213,  ..., 0.0632, 0.0305, 0.0756],\n",
            "          [0.0629, 0.0294, 0.0376,  ..., 0.0324, 0.0381, 0.0474],\n",
            "          ...,\n",
            "          [0.0545, 0.0629, 0.0773,  ..., 0.0416, 0.0441, 0.0773],\n",
            "          [0.0655, 0.0344, 0.0615,  ..., 0.0525, 0.0477, 0.0561],\n",
            "          [0.0726, 0.0449, 0.0455,  ..., 0.0533, 0.0391, 0.0305]],\n",
            "\n",
            "         [[0.0377, 0.0277, 0.0314,  ..., 0.0286, 0.0641, 0.0330],\n",
            "          [0.0556, 0.0303, 0.0771,  ..., 0.0440, 0.0383, 0.0460],\n",
            "          [0.0490, 0.0354, 0.0401,  ..., 0.0899, 0.0387, 0.0585],\n",
            "          ...,\n",
            "          [0.0342, 0.0265, 0.0359,  ..., 0.0445, 0.0435, 0.0312],\n",
            "          [0.0382, 0.0276, 0.0564,  ..., 0.0347, 0.0461, 0.0630],\n",
            "          [0.0304, 0.0275, 0.0552,  ..., 0.0562, 0.0717, 0.0425]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0570, 0.0476, 0.0332,  ..., 0.0387, 0.0451, 0.0594],\n",
            "          [0.0555, 0.0390, 0.0621,  ..., 0.0502, 0.0426, 0.0456],\n",
            "          [0.0217, 0.0473, 0.1254,  ..., 0.0701, 0.0482, 0.0245],\n",
            "          ...,\n",
            "          [0.0438, 0.0593, 0.0551,  ..., 0.0484, 0.0964, 0.0403],\n",
            "          [0.0399, 0.0421, 0.0760,  ..., 0.0526, 0.0668, 0.0512],\n",
            "          [0.0306, 0.0403, 0.0780,  ..., 0.0578, 0.0613, 0.0380]],\n",
            "\n",
            "         [[0.0529, 0.0378, 0.0309,  ..., 0.0977, 0.0634, 0.0264],\n",
            "          [0.0493, 0.0610, 0.0625,  ..., 0.0524, 0.0394, 0.0468],\n",
            "          [0.0350, 0.0436, 0.0497,  ..., 0.0341, 0.0384, 0.0664],\n",
            "          ...,\n",
            "          [0.0326, 0.0594, 0.0597,  ..., 0.0643, 0.0345, 0.0775],\n",
            "          [0.0548, 0.0755, 0.0532,  ..., 0.0474, 0.0823, 0.0385],\n",
            "          [0.0283, 0.0350, 0.0628,  ..., 0.0385, 0.0354, 0.1186]],\n",
            "\n",
            "         [[0.0211, 0.0490, 0.0506,  ..., 0.0751, 0.0727, 0.0377],\n",
            "          [0.0509, 0.0282, 0.0450,  ..., 0.0723, 0.0532, 0.0217],\n",
            "          [0.0625, 0.0495, 0.0466,  ..., 0.0543, 0.0663, 0.0320],\n",
            "          ...,\n",
            "          [0.0397, 0.0321, 0.0816,  ..., 0.0501, 0.0519, 0.0381],\n",
            "          [0.0426, 0.0536, 0.0488,  ..., 0.0411, 0.0491, 0.0658],\n",
            "          [0.0475, 0.0326, 0.0359,  ..., 0.0483, 0.0410, 0.0592]]],\n",
            "\n",
            "\n",
            "        [[[0.0571, 0.0351, 0.0376,  ..., 0.0598, 0.0598, 0.0598],\n",
            "          [0.0365, 0.0367, 0.0698,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0326, 0.0404, 0.0441,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          ...,\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543]],\n",
            "\n",
            "         [[0.0402, 0.0318, 0.0618,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0411, 0.0366, 0.0406,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0432, 0.0376, 0.0534,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          ...,\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543]],\n",
            "\n",
            "         [[0.0393, 0.0791, 0.0315,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0448, 0.0678, 0.0431,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0399, 0.0686, 0.0424,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          ...,\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0789, 0.0462, 0.0710,  ..., 0.0191, 0.0191, 0.0191],\n",
            "          [0.0535, 0.0519, 0.0901,  ..., 0.0288, 0.0288, 0.0288],\n",
            "          [0.0454, 0.0511, 0.0363,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          ...,\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567]],\n",
            "\n",
            "         [[0.0702, 0.0637, 0.0403,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0769, 0.0389, 0.0429,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0272, 0.0450, 0.0356,  ..., 0.0747, 0.0747, 0.0747],\n",
            "          ...,\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645],\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645],\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645]],\n",
            "\n",
            "         [[0.0874, 0.0660, 0.0550,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0483, 0.0596, 0.0646,  ..., 0.0261, 0.0261, 0.0261],\n",
            "          [0.0557, 0.0307, 0.0536,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          ...,\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434]]],\n",
            "\n",
            "\n",
            "        [[[0.0335, 0.0415, 0.0440,  ..., 0.0706, 0.0706, 0.0706],\n",
            "          [0.0441, 0.0396, 0.0489,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0412, 0.0673, 0.0596,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          ...,\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577]],\n",
            "\n",
            "         [[0.0560, 0.0376, 0.0757,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0394, 0.0360, 0.0400,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0503, 0.0340, 0.0552,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          ...,\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564],\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564],\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564]],\n",
            "\n",
            "         [[0.0327, 0.0391, 0.0349,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0459, 0.0684, 0.0274,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          [0.0461, 0.0740, 0.0627,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          ...,\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0367, 0.0556, 0.0599,  ..., 0.0392, 0.0392, 0.0392],\n",
            "          [0.0727, 0.0527, 0.0678,  ..., 0.0293, 0.0293, 0.0293],\n",
            "          [0.0528, 0.0507, 0.0694,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          ...,\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571]],\n",
            "\n",
            "         [[0.0409, 0.0640, 0.0423,  ..., 0.0587, 0.0587, 0.0587],\n",
            "          [0.0420, 0.0381, 0.0413,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0427, 0.0466, 0.0632,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          ...,\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600]],\n",
            "\n",
            "         [[0.0606, 0.0541, 0.0623,  ..., 0.0354, 0.0354, 0.0354],\n",
            "          [0.0808, 0.0584, 0.0598,  ..., 0.0255, 0.0255, 0.0255],\n",
            "          [0.0439, 0.0522, 0.0787,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          ...,\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy6D23tZ7icl"
      },
      "source": [
        "```python\n",
        "이후 계산된 attention 값을 v과 곱하여 최종 결괏값을 제시합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-OX-ddDQ7icl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515421f9-d2c0-465d-b950-b372aad2900e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGC1aj667icl"
      },
      "source": [
        "### 각 head의 결과 병합(concat)\n",
        "```python\n",
        "각 head의 결과물을 concat하고 동일 차원(d_model)으로 linear transformation 합니다.\n",
        "\n",
        "여기서 'd_model' 차원으로 linear transformation 하는 이유는 transformer 모델에서 원래의 데이터와 더하는 연산(residual connection)이 존재하여 이때 차원을 통일해야 하기 때문입니다.\n",
        "\n",
        "residual connection 연산은 아래 이미지에서 Self-Attention 블록 이후 Add에 해당하는 연산입니다.\n",
        "\n",
        "residual connection은 앞선 강의에서 배운 resnet에서 소개된 기술입니다.\n",
        "```\n",
        "![residual](https://github.com/Pjunn/GDSC_mlstudy/blob/main/7%EC%A3%BC%EC%B0%A8/transformer_resideual_layer_norm.png?raw=true)\n",
        "이미지 출처: https://jalammar.github.io/illustrated-transformer/ <br><br>\n",
        "-[What is Residual Connection?](https://paperswithcode.com/method/residual-connection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TDcwSJXq7icv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d3d062-be69-4ef7-fbfc-542af5365508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R2YMaNuT7icv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62782e8-81e6-42e7-a101-6bc0de1aa367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.0647e-01,  1.7614e-02,  1.5888e-02,  ..., -7.6811e-02,\n",
            "           4.7090e-02,  5.9217e-02],\n",
            "         [ 1.8358e-01,  6.6134e-02,  4.2202e-02,  ..., -7.3667e-02,\n",
            "           7.7152e-02,  7.9882e-02],\n",
            "         [ 2.3217e-01,  7.2016e-02,  2.5100e-02,  ..., -1.0642e-01,\n",
            "           6.3860e-02,  7.2267e-02],\n",
            "         ...,\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02],\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02],\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02]],\n",
            "\n",
            "        [[ 3.5702e-01, -1.7859e-01,  3.9717e-02,  ...,  2.8024e-02,\n",
            "          -3.8113e-02,  8.8643e-02],\n",
            "         [ 3.9739e-01, -1.5972e-01,  7.6489e-02,  ...,  5.3090e-02,\n",
            "          -6.2734e-06,  2.7271e-02],\n",
            "         [ 4.0024e-01, -1.7384e-01,  5.6186e-02,  ...,  7.0403e-02,\n",
            "          -8.3607e-03,  3.8790e-02],\n",
            "         ...,\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02],\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02],\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02]],\n",
            "\n",
            "        [[ 1.8888e-01, -1.0315e-01,  4.6053e-02,  ...,  2.9630e-02,\n",
            "          -3.4317e-02, -3.9398e-02],\n",
            "         [ 2.3809e-01, -1.7388e-01,  4.3160e-03,  ..., -5.1572e-02,\n",
            "          -1.9559e-02, -7.7400e-02],\n",
            "         [ 2.1433e-01, -1.3635e-01,  7.1454e-03,  ..., -3.7201e-02,\n",
            "          -3.4568e-02, -1.1389e-01],\n",
            "         ...,\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02],\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02],\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.1422e-01, -1.5383e-01,  1.4641e-02,  ..., -1.6093e-01,\n",
            "          -8.8677e-03, -9.0119e-02],\n",
            "         [-1.2518e-01, -1.3702e-01,  5.0213e-02,  ..., -1.6496e-01,\n",
            "           3.1292e-02, -8.7259e-02],\n",
            "         [-1.0011e-01, -1.3629e-01,  5.5492e-02,  ..., -1.1238e-01,\n",
            "          -2.9258e-03, -7.9936e-02],\n",
            "         ...,\n",
            "         [-1.2910e-01, -1.3411e-01,  1.2819e-03,  ..., -1.6582e-01,\n",
            "           4.9708e-02, -9.1370e-02],\n",
            "         [-1.1669e-01, -1.5108e-01,  5.2810e-02,  ..., -1.6697e-01,\n",
            "           7.2773e-02, -7.3678e-02],\n",
            "         [-1.3154e-01, -1.4187e-01,  1.2646e-02,  ..., -1.4920e-01,\n",
            "           3.7681e-03, -1.0389e-01]],\n",
            "\n",
            "        [[ 8.0946e-02, -9.6081e-03,  4.2926e-03,  ..., -1.3398e-01,\n",
            "          -4.1658e-02, -3.3607e-02],\n",
            "         [ 6.0193e-02, -9.4748e-04, -3.4700e-02,  ..., -1.6770e-01,\n",
            "          -2.4062e-02, -5.0679e-02],\n",
            "         [ 6.7370e-02,  4.0241e-02, -1.1831e-02,  ..., -1.5772e-01,\n",
            "          -2.6232e-02, -4.7158e-02],\n",
            "         ...,\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02],\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02],\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02]],\n",
            "\n",
            "        [[ 1.5702e-01, -1.3273e-01, -1.5563e-01,  ..., -3.5365e-02,\n",
            "           4.6784e-02,  3.2669e-02],\n",
            "         [ 1.2019e-01, -1.2637e-01, -1.8690e-01,  ..., -9.1197e-02,\n",
            "           4.7907e-03,  7.1274e-02],\n",
            "         [ 1.2186e-01, -8.5784e-02, -1.6949e-01,  ..., -2.0013e-02,\n",
            "           2.7023e-02,  6.2415e-02],\n",
            "         ...,\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02],\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02],\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OzymRkf7icv"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현해 봅시다.\n",
        "```python\n",
        "🐙\n",
        "아래의 Multi-head attention 모듈에서 '#TODO'를 채워 모듈을 완성 시켜주세요.\n",
        "위 실습에서 배운 내용이 큰 힌트가 될 거예요!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gWcXvPm37icw"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, dim_model, num_heads):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "    assert dim_model % num_heads == 0\n",
        "\n",
        "    self.dim_model = dim_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = dim_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(dim_model, dim_model)\n",
        "    self.w_k = nn.Linear(dim_model, dim_model)\n",
        "    self.w_v = nn.Linear(dim_model, dim_model)\n",
        "    self.w_0 = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    q = self.w_q(query)\n",
        "    k = self.w_k(key)\n",
        "    v = self.w_v(value)\n",
        "\n",
        "    attn_values = self.self_attention(q,k,v)\n",
        "\n",
        "    outputs = attn_values.view(-1, query.size(1), query.size(2))\n",
        "    outputs = self.w_0(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def self_attention(self,q,k,v):\n",
        "      scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k) # (B,n_head,L,L)\n",
        "      attn_dists = F.softmax(scores,dim=-1)\n",
        "      output = torch.matmul(attn_dists, v).transpose(1, 2).contiguous().view(q.shape[0], -1, d_model)\n",
        "\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Lvl4um7icw"
      },
      "outputs": [],
      "source": [
        "dim_model = 512\n",
        "num_heads = 8\n",
        "multihead_attn = MultiheadAttention(dim_model, num_heads)\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zd6I8vxD7icw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91781751-27b3-4065-9c5e-d038e9f0e403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ]
        }
      ],
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if outputs.shape == batch_emb.shape:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"🐙 다시 도전해봐요!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}