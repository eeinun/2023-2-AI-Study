{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAYPwLK37icX"
      },
      "source": [
        "# NLP 2 ê³¼ì œ\n",
        "> ì¸ê³µì§€ëŠ¥ ìŠ¤í„°ë”” ì¼ê³± ë²ˆì§¸ ê³¼ì œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ë°°ìš´ ë‹¤ì–‘í•œ ì§€ì‹ë“¤ì„ ì‹¤ìŠµì„ í†µí•´ì„œ í™œìš©í•´ ë³¼ ì‹œê°„ì„ ê°€ì§ˆ ê²ƒì…ë‹ˆë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDFP_3m7ica"
      },
      "source": [
        "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Transformer\n",
        "\n",
        "ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ì´ ê³„ì‚°ë˜ëŠ” multi-head attentionì—ì„œ query, key, value ë²¡í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ\n",
        "projection matrix $(â€¯W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}â€‹â€‹â€¯)$ëŠ” head ê°„ì— sharing ëœë‹¤. <br>\n",
        "***\n",
        "$â€¯MultiHead(Q,K,V)=Concat(head_1, \\cdots, head_h)W^{O} $ (ì´ë•Œ, $W^{O}$ ëŠ” Outputì„ ë§Œë“¤ë•Œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬)<br>\n",
        "where $head_i=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$ (ì´ë•Œ, $Q, K, V$ ëŠ” ì…ë ¥ì—ì„œ tokenizeëœ ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ë²¡í„° $Q = K = V$ )\n",
        "```python\n",
        "(1) ì˜ˆ\n",
        "(2) ì•„ë‹ˆì˜¤\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG6Ti5SW7icb"
      },
      "source": [
        "```python\n",
        "ğŸ˜‰\n",
        "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmP7gTWF7icb"
      },
      "source": [
        "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Transformer\n",
        "```python\n",
        "Transformer ëª¨ë¸ì—ì„œ ê° ì…ë ¥ í† í°ë“¤ì´ ê°€ì§„ ìˆœì„œë¥¼ ì…ë ¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ê³ ë¥´ì‹œì˜¤.\n",
        "\n",
        "(1) Positional Encoding\n",
        "(2) Encoder-Decoder attention\n",
        "(3) Layer normalization\n",
        "(4) Masked decoder self-attention\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5_XGnos7icc"
      },
      "source": [
        "```python\n",
        "ğŸ˜‰\n",
        "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
        "1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-1a8qNT7icc"
      },
      "source": [
        "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 ëª¨ë¸ì´ ì–´ë–»ê²Œ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4BU3MK7icc"
      },
      "source": [
        "```python\n",
        "ğŸ˜‰\n",
        "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
        "ë…¼ë¦¬ì  í•¨ì˜ ì¶”ì¸¡, ìœ ì‚¬ì„± ê²€ì‚¬, ë‹¤ì§€ì„ ë‹¤ë¬¸ì œ ì •ë‹µ ì„ íƒ\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DUSVATt7icc"
      },
      "source": [
        "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> GPT\n",
        "```python\n",
        "GPT-1 ëª¨ë¸ì˜ \"GPT\" ì•½ìëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?\n",
        "\n",
        "(1) Generalized Pre-trained Transformer\n",
        "(2) Generative Pre-trained Transformer\n",
        "(3) Globalized Pre-processing Transformer\n",
        "(4) Gradient Propagation Technique\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7itJhcZ7icc"
      },
      "source": [
        "```python\n",
        "ğŸ˜‰\n",
        "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQkRGcFB7icd"
      },
      "source": [
        "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> BERT\n",
        "```python\n",
        "ë‹¤ìŒ ì¤‘ BERTì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì„ ê³ ë¥´ì‹œì˜¤.\n",
        "\n",
        "(1) í•™ìŠµ ë°ì´í„°ì—ì„œ [MASK] í† í°ì´ ì„ íƒë˜ëŠ” ë¹„ìœ¨ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ì€ ê²½ìš°, ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¹„ìš©ì´ ì¦ê°€í•œë‹¤.\n",
        "(2) Unidirectional modelë¡œ ìì—°ì–´ ìƒì„±ì— íŠ¹í™”ëœ ëª¨ë¸ì´ë‹¤.\n",
        "(3) ì…ë ¥ ì‹œí€€ìŠ¤ ì¤‘ ì¼ë¶€ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ë§ì¶”ëŠ” masked language modeling (masked LM)ì„ í†µí•´ pre-trainingì„ ìˆ˜í–‰í•˜ì˜€ë‹¤.\n",
        "(4) ì‚¬ì „í•™ìŠµì„ ìœ„í•œ [MASK] í† í°ì€ randomí•˜ê²Œ ì„ íƒëœë‹¤.\n",
        "(5) Unlabeled ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ self-supervised learningì„ ì ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ì´ë‹¤.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0-9QdQa7icd"
      },
      "source": [
        "```python\n",
        "ğŸ˜‰\n",
        "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
        "2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQrt9UOF7icd"
      },
      "source": [
        "#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì‹¤ìŠµ ]</b></font> Multi-head Attention\n",
        "```python\n",
        "ì´ë²ˆ ì‹¤ìŠµì„ í†µí•´ ë‹¤ìŒ 2ê°€ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
        "1. Multi-head attention ë° self-attentionì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "2. ê° ê³¼ì •ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì—°ì‚°ê³¼ input/output í˜•íƒœë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJjr2VpE7ice"
      },
      "source": [
        "```python\n",
        "ğŸ™\n",
        "ë¨¼ì € ì½”ë“œ ì‹¤í–‰ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ import í•´ë´…ì‹œë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lnnhjIo37ice"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52v_vrwh7icf"
      },
      "source": [
        "### ë°ì´í„° ì „ì²˜ë¦¬\n",
        "```python\n",
        "ì €ë²ˆ ì£¼ì°¨ì˜ ë°ì´í„°ì™€ ë¹„ìŠ·í•œ í˜•íƒœì…ë‹ˆë‹¤.\n",
        "ë¨¼ì € ì „ì²´ ë‹¨ì–´ ìˆ˜ì¸ vocab_sizeê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.\n",
        "pad_idëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì§„í–‰í•˜ê²Œ ë˜ëŠ”ë° ì´ë•Œ íŒ¨ë”©ì„ ì˜ë¯¸í•˜ëŠ” í† í°ì˜ idì…ë‹ˆë‹¤.\n",
        "sample data ë³´ë©´ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” ì €í¬ê°€ êµ¬ì„±í•œ vocabì—ì„œ ëª‡ ë²ˆì§¸ ë‹¨ì–´ì¸ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "ë”°ë¼ì„œ ë°ì´í„°ì˜ ê° ìš”ì†Œë¥¼ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hgrXjEpJ7icg"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100\n",
        "pad_id = 0\n",
        "\n",
        "data = [\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
        "  [60, 96, 51, 32, 90],\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
        "  [75, 51],\n",
        "  [66, 88, 98, 47],\n",
        "  [21, 39, 10, 64, 21],\n",
        "  [98],\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO396mcz7icg"
      },
      "source": [
        "```python\n",
        "ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•œ padding í•¨ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MdQdZmXL7icg"
      },
      "outputs": [],
      "source": [
        "def padding(data):\n",
        "  max_len = len(max(data, key=len))\n",
        "  print(f\"Maximum sequence length: {max_len}\")\n",
        "\n",
        "  for i, seq in enumerate(tqdm(data)):\n",
        "    if len(seq) < max_len:\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
        "\n",
        "  return data, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzUALIKw7ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2446384-b00d-421a-fa75-3d50bc5de2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 62508.26it/s]\n"
          ]
        }
      ],
      "source": [
        "data, max_len = padding(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA7OYDMF7ici"
      },
      "source": [
        "```python\n",
        "ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ í™•ì¸í•´ ë³´ë©´ ì˜ íŒ¨ë”© ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lZbTruS77ici",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa29a62e-62e3-446b-9973-ac09c6316190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGnTiEqm7icj"
      },
      "source": [
        "### Hyperparameter ì„¸íŒ… ë° embedding\n",
        "```python\n",
        "ìœ„ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ ì‹¤ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2Ktj2JXZ7icj"
      },
      "outputs": [],
      "source": [
        "d_model = 512  # modelì˜ hidden size\n",
        "num_heads = 8  # multi-headì—ì„œì˜ headì˜ ê°œìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JdtJv_mK7icj"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "# B: ë°°ì¹˜ ì‚¬ì´ì¦ˆ, L: maximum sequence length\n",
        "batch = torch.LongTensor(data)  # (B, L)\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lP8s6bSE7icj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e20d5f-decb-4aa8-ced8-1a7d81f1f29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4282,  0.6188,  0.8736,  ..., -0.8371, -0.5100,  0.7989],\n",
            "         [-1.9735,  0.2280,  0.0281,  ...,  2.0192,  0.1319,  0.0241],\n",
            "         [ 0.7195,  0.2482,  1.1721,  ...,  1.1802,  0.2993,  1.6275],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[-2.1553,  1.5210,  0.4982,  ..., -1.4450,  2.2981, -0.1059],\n",
            "         [-0.7475, -0.3579, -0.2764,  ...,  0.5888, -1.6453,  1.4642],\n",
            "         [ 0.3176, -1.8282,  1.1573,  ..., -0.2372, -1.3562, -0.9574],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[-1.3934, -0.3462, -0.8814,  ...,  1.1212, -1.7532,  0.0119],\n",
            "         [ 0.4660,  0.4771,  0.2235,  ...,  0.5900,  0.5305,  0.4062],\n",
            "         [-0.1280,  0.4496,  0.8122,  ...,  0.0750,  0.5611,  0.3438],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7640,  0.3421, -3.3202,  ...,  2.4083,  1.0090,  1.0383],\n",
            "         [-0.1898,  1.1985,  0.3842,  ...,  2.2184, -1.4027, -0.8913],\n",
            "         [ 0.3176, -1.8282,  1.1573,  ..., -0.2372, -1.3562, -0.9574],\n",
            "         ...,\n",
            "         [ 1.3513,  0.6911, -1.0318,  ...,  0.5124,  0.4218, -3.4490],\n",
            "         [ 1.4607,  0.3821,  0.8196,  ..., -0.0147, -1.6442,  0.0448],\n",
            "         [ 0.1682, -0.9391,  2.3732,  ...,  0.7258, -0.1115,  1.4364]],\n",
            "\n",
            "        [[-0.1201,  0.7537, -0.9623,  ...,  1.5528,  0.3273,  2.0615],\n",
            "         [-0.5071,  1.2079,  0.6430,  ...,  0.1678, -0.8953,  1.3479],\n",
            "         [ 0.8689, -1.1293, -0.4145,  ..., -1.0937,  0.5434, -0.4408],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]],\n",
            "\n",
            "        [[ 0.2676,  1.6211, -1.3688,  ..., -0.2610,  0.2634,  0.7333],\n",
            "         [-0.5071,  1.2079,  0.6430,  ...,  0.1678, -0.8953,  1.3479],\n",
            "         [ 1.1269, -1.8938,  1.1857,  ..., -0.0399, -0.2727, -2.2429],\n",
            "         ...,\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408],\n",
            "         [ 0.2835,  0.4679,  0.8529,  ..., -1.3343, -1.2773, -0.9408]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "print(batch_emb)\n",
        "print(batch_emb.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgjjqmOP7icj"
      },
      "source": [
        "### Linear transformation & ì—¬ëŸ¬ headë¡œ ë‚˜ëˆ„ê¸°\n",
        "```python\n",
        "Multi-head attention ë‚´ì—ì„œ ì“°ì´ëŠ” linear transformation matrixë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "query, key, valueë¥¼ ì„œë¡œ ë‹¤ë¥¸ linear transformation matrixë¡œ í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ ë™ì¼í•œ ë°ì´í„°(batch_emb)ë¡œë¶€í„° ì„œë¡œ ë‹¤ë¥¸ query, key, valueë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9bZJNvK57icj"
      },
      "outputs": [],
      "source": [
        "w_q = nn.Linear(d_model, d_model)\n",
        "w_k = nn.Linear(d_model, d_model)\n",
        "w_v = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9RzeDr_7icj"
      },
      "source": [
        "```python\n",
        "output layerì—ì„œ ì‚¬ìš©ë  í–‰ë ¬ë„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7b7BwyqA7ick"
      },
      "outputs": [],
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P_kCGno67ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc347e2-4b50-476a-8a15-81c79085b7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO2e0nJd7ick"
      },
      "source": [
        "```python\n",
        "q, k, vë¥¼ 'num_head' ê°œì˜ ì°¨ì›ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "ì‹¤ì œ q, k, v ê°ê°ì˜ ë²¡í„° í¬ê¸°ëŠ” 512ê°€ ì•„ë‹Œ 64ì…ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LtSA5ltJ7ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea4014c-9bf6-4d58-e926-c9a6c826bc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ]
        }
      ],
      "source": [
        "batch_size = q.shape[0]\n",
        "d_k = d_model // num_heads # q, k, v ë²¡í„° ì‚¬ì´ì¦ˆ\n",
        "\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QEGqCSE7ick"
      },
      "source": [
        "```python\n",
        "8ê°œì˜ headì— í•„ìš”í•œ q, k, vê°€ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oijExEqd7ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e98ddcd-ac5c-46e8-ae8f-0cab7c236496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(q.shape)\n",
        "print(k.shape)\n",
        "print(v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTefr-GC7ick"
      },
      "source": [
        "### Scaled dot-product self-attention êµ¬í˜„\n",
        "```python\n",
        "ê° headì—ì„œ ì‹¤í–‰ë˜ëŠ” self-attention ê³¼ì •ì„ ì‚´í´ë´…ì‹œë‹¤.\n",
        "\n",
        "q, k ë²¡í„°ì˜ ë‚´ì  ì—°ì‚° ì´í›„ì— d_kì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.\n",
        "ì´ëŠ” qì™€ kë¥¼ êµ¬ì„±í•˜ëŠ” ìš”ì†Œì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ë‚´ì ì˜ ê²°ê´ê°’ì— ëŒ€í•´ì„œë„ ìœ ì§€ì‹œì¼œì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì´í›„ ê³„ì‚°ëœ ê° í–‰ì— ëŒ€í•´ì„œ softmax ì—°ì‚°ì„ í†µí•´ì„œ ê° ìš”ì†Œì˜ í•©ì„ 1ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
        "```\n",
        "- [Scaled Dot-Product Attention ì°¸ê³ ](https://paperswithcode.com/method/scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "71QTPtz27ick",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cb9fce-2ff6-42b6-97c5-1cdfe4ff588b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0330, 0.0564, 0.0388,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          [0.0461, 0.0785, 0.0470,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0649, 0.0714, 0.0281,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502],\n",
            "          [0.0378, 0.0415, 0.0701,  ..., 0.0502, 0.0502, 0.0502]],\n",
            "\n",
            "         [[0.0437, 0.0439, 0.0536,  ..., 0.0466, 0.0466, 0.0466],\n",
            "          [0.0360, 0.0788, 0.0283,  ..., 0.0508, 0.0508, 0.0508],\n",
            "          [0.0556, 0.0511, 0.0556,  ..., 0.0346, 0.0346, 0.0346],\n",
            "          ...,\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599],\n",
            "          [0.0448, 0.0458, 0.0502,  ..., 0.0599, 0.0599, 0.0599]],\n",
            "\n",
            "         [[0.0294, 0.0393, 0.0628,  ..., 0.0620, 0.0620, 0.0620],\n",
            "          [0.0606, 0.0325, 0.0830,  ..., 0.0384, 0.0384, 0.0384],\n",
            "          [0.0884, 0.0367, 0.0656,  ..., 0.0298, 0.0298, 0.0298],\n",
            "          ...,\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447],\n",
            "          [0.0406, 0.0549, 0.0419,  ..., 0.0447, 0.0447, 0.0447]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0417, 0.0476, 0.0513,  ..., 0.0660, 0.0660, 0.0660],\n",
            "          [0.0614, 0.0197, 0.0377,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0847, 0.0524, 0.0574,  ..., 0.0288, 0.0288, 0.0288],\n",
            "          ...,\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563],\n",
            "          [0.0504, 0.0374, 0.0817,  ..., 0.0563, 0.0563, 0.0563]],\n",
            "\n",
            "         [[0.0562, 0.0396, 0.0660,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          [0.0966, 0.0669, 0.0301,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0665, 0.0878, 0.0604,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          ...,\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632],\n",
            "          [0.0224, 0.0400, 0.0634,  ..., 0.0632, 0.0632, 0.0632]],\n",
            "\n",
            "         [[0.0358, 0.0319, 0.0813,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          [0.0611, 0.0504, 0.0442,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0409, 0.0609, 0.0632,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          ...,\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0763, 0.0655, 0.0499,  ..., 0.0408, 0.0408, 0.0408]]],\n",
            "\n",
            "\n",
            "        [[[0.0415, 0.0281, 0.0190,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0505, 0.0608, 0.0549,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0272, 0.0170, 0.0289,  ..., 0.0554, 0.0554, 0.0554],\n",
            "          ...,\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524],\n",
            "          [0.0188, 0.0468, 0.0541,  ..., 0.0524, 0.0524, 0.0524]],\n",
            "\n",
            "         [[0.0143, 0.0378, 0.0391,  ..., 0.0542, 0.0542, 0.0542],\n",
            "          [0.0604, 0.0837, 0.0771,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0676, 0.1128, 0.0509,  ..., 0.0438, 0.0438, 0.0438],\n",
            "          ...,\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0393, 0.0218, 0.0474,  ..., 0.0520, 0.0520, 0.0520]],\n",
            "\n",
            "         [[0.0371, 0.0558, 0.0513,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0379, 0.0233, 0.0323,  ..., 0.0552, 0.0552, 0.0552],\n",
            "          [0.0408, 0.0253, 0.0634,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          ...,\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0416, 0.0723, 0.0510,  ..., 0.0483, 0.0483, 0.0483]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1056, 0.0736, 0.0573,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          [0.0468, 0.0961, 0.0293,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0290, 0.0894, 0.1124,  ..., 0.0457, 0.0457, 0.0457],\n",
            "          ...,\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504],\n",
            "          [0.0511, 0.0477, 0.0558,  ..., 0.0504, 0.0504, 0.0504]],\n",
            "\n",
            "         [[0.0458, 0.0521, 0.0491,  ..., 0.0498, 0.0498, 0.0498],\n",
            "          [0.0417, 0.0359, 0.0399,  ..., 0.0493, 0.0493, 0.0493],\n",
            "          [0.0657, 0.0442, 0.0607,  ..., 0.0456, 0.0456, 0.0456],\n",
            "          ...,\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0461, 0.0521, 0.0480,  ..., 0.0515, 0.0515, 0.0515]],\n",
            "\n",
            "         [[0.0634, 0.0417, 0.0503,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0314, 0.0303, 0.0405,  ..., 0.0556, 0.0556, 0.0556],\n",
            "          [0.0757, 0.0511, 0.0386,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          ...,\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485],\n",
            "          [0.0494, 0.0552, 0.0336,  ..., 0.0485, 0.0485, 0.0485]]],\n",
            "\n",
            "\n",
            "        [[[0.0592, 0.0739, 0.0699,  ..., 0.0326, 0.0326, 0.0326],\n",
            "          [0.0372, 0.0330, 0.0410,  ..., 0.0531, 0.0531, 0.0531],\n",
            "          [0.0353, 0.0840, 0.0262,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          ...,\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584],\n",
            "          [0.0315, 0.0323, 0.0402,  ..., 0.0584, 0.0584, 0.0584]],\n",
            "\n",
            "         [[0.0548, 0.0345, 0.0281,  ..., 0.0587, 0.0587, 0.0587],\n",
            "          [0.0453, 0.0479, 0.0493,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0729, 0.0867, 0.0544,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          ...,\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0389, 0.0462, 0.0288,  ..., 0.0512, 0.0512, 0.0512]],\n",
            "\n",
            "         [[0.0319, 0.0654, 0.0487,  ..., 0.0520, 0.0520, 0.0520],\n",
            "          [0.0632, 0.0313, 0.0610,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0484, 0.0514, 0.0553,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          ...,\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0446, 0.0499, 0.0614,  ..., 0.0427, 0.0427, 0.0427]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0494, 0.0547, 0.0254,  ..., 0.0608, 0.0608, 0.0608],\n",
            "          [0.0885, 0.0538, 0.0580,  ..., 0.0411, 0.0411, 0.0411],\n",
            "          [0.0653, 0.0466, 0.0339,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          ...,\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528],\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528],\n",
            "          [0.0776, 0.0378, 0.0531,  ..., 0.0528, 0.0528, 0.0528]],\n",
            "\n",
            "         [[0.0583, 0.0246, 0.0442,  ..., 0.0611, 0.0611, 0.0611],\n",
            "          [0.0374, 0.0476, 0.0551,  ..., 0.0522, 0.0522, 0.0522],\n",
            "          [0.0581, 0.0287, 0.0490,  ..., 0.0613, 0.0613, 0.0613],\n",
            "          ...,\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614],\n",
            "          [0.0491, 0.0553, 0.0452,  ..., 0.0614, 0.0614, 0.0614]],\n",
            "\n",
            "         [[0.0311, 0.0397, 0.0577,  ..., 0.0534, 0.0534, 0.0534],\n",
            "          [0.0430, 0.0879, 0.0702,  ..., 0.0490, 0.0490, 0.0490],\n",
            "          [0.0386, 0.0507, 0.0432,  ..., 0.0399, 0.0399, 0.0399],\n",
            "          ...,\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448],\n",
            "          [0.0904, 0.0255, 0.0714,  ..., 0.0448, 0.0448, 0.0448]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0478, 0.0240, 0.0395,  ..., 0.0389, 0.0493, 0.0322],\n",
            "          [0.0697, 0.0500, 0.0320,  ..., 0.0430, 0.0426, 0.0462],\n",
            "          [0.1078, 0.0369, 0.0356,  ..., 0.0398, 0.0786, 0.0388],\n",
            "          ...,\n",
            "          [0.0785, 0.0629, 0.0371,  ..., 0.0374, 0.0293, 0.0498],\n",
            "          [0.0316, 0.0699, 0.0479,  ..., 0.0574, 0.0314, 0.0550],\n",
            "          [0.0690, 0.0518, 0.0455,  ..., 0.0363, 0.0475, 0.0501]],\n",
            "\n",
            "         [[0.0535, 0.0527, 0.0548,  ..., 0.0577, 0.0478, 0.0417],\n",
            "          [0.0372, 0.0363, 0.0213,  ..., 0.0632, 0.0305, 0.0756],\n",
            "          [0.0629, 0.0294, 0.0376,  ..., 0.0324, 0.0381, 0.0474],\n",
            "          ...,\n",
            "          [0.0545, 0.0629, 0.0773,  ..., 0.0416, 0.0441, 0.0773],\n",
            "          [0.0655, 0.0344, 0.0615,  ..., 0.0525, 0.0477, 0.0561],\n",
            "          [0.0726, 0.0449, 0.0455,  ..., 0.0533, 0.0391, 0.0305]],\n",
            "\n",
            "         [[0.0377, 0.0277, 0.0314,  ..., 0.0286, 0.0641, 0.0330],\n",
            "          [0.0556, 0.0303, 0.0771,  ..., 0.0440, 0.0383, 0.0460],\n",
            "          [0.0490, 0.0354, 0.0401,  ..., 0.0899, 0.0387, 0.0585],\n",
            "          ...,\n",
            "          [0.0342, 0.0265, 0.0359,  ..., 0.0445, 0.0435, 0.0312],\n",
            "          [0.0382, 0.0276, 0.0564,  ..., 0.0347, 0.0461, 0.0630],\n",
            "          [0.0304, 0.0275, 0.0552,  ..., 0.0562, 0.0717, 0.0425]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0570, 0.0476, 0.0332,  ..., 0.0387, 0.0451, 0.0594],\n",
            "          [0.0555, 0.0390, 0.0621,  ..., 0.0502, 0.0426, 0.0456],\n",
            "          [0.0217, 0.0473, 0.1254,  ..., 0.0701, 0.0482, 0.0245],\n",
            "          ...,\n",
            "          [0.0438, 0.0593, 0.0551,  ..., 0.0484, 0.0964, 0.0403],\n",
            "          [0.0399, 0.0421, 0.0760,  ..., 0.0526, 0.0668, 0.0512],\n",
            "          [0.0306, 0.0403, 0.0780,  ..., 0.0578, 0.0613, 0.0380]],\n",
            "\n",
            "         [[0.0529, 0.0378, 0.0309,  ..., 0.0977, 0.0634, 0.0264],\n",
            "          [0.0493, 0.0610, 0.0625,  ..., 0.0524, 0.0394, 0.0468],\n",
            "          [0.0350, 0.0436, 0.0497,  ..., 0.0341, 0.0384, 0.0664],\n",
            "          ...,\n",
            "          [0.0326, 0.0594, 0.0597,  ..., 0.0643, 0.0345, 0.0775],\n",
            "          [0.0548, 0.0755, 0.0532,  ..., 0.0474, 0.0823, 0.0385],\n",
            "          [0.0283, 0.0350, 0.0628,  ..., 0.0385, 0.0354, 0.1186]],\n",
            "\n",
            "         [[0.0211, 0.0490, 0.0506,  ..., 0.0751, 0.0727, 0.0377],\n",
            "          [0.0509, 0.0282, 0.0450,  ..., 0.0723, 0.0532, 0.0217],\n",
            "          [0.0625, 0.0495, 0.0466,  ..., 0.0543, 0.0663, 0.0320],\n",
            "          ...,\n",
            "          [0.0397, 0.0321, 0.0816,  ..., 0.0501, 0.0519, 0.0381],\n",
            "          [0.0426, 0.0536, 0.0488,  ..., 0.0411, 0.0491, 0.0658],\n",
            "          [0.0475, 0.0326, 0.0359,  ..., 0.0483, 0.0410, 0.0592]]],\n",
            "\n",
            "\n",
            "        [[[0.0571, 0.0351, 0.0376,  ..., 0.0598, 0.0598, 0.0598],\n",
            "          [0.0365, 0.0367, 0.0698,  ..., 0.0527, 0.0527, 0.0527],\n",
            "          [0.0326, 0.0404, 0.0441,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          ...,\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0368, 0.0233, 0.0562,  ..., 0.0543, 0.0543, 0.0543]],\n",
            "\n",
            "         [[0.0402, 0.0318, 0.0618,  ..., 0.0539, 0.0539, 0.0539],\n",
            "          [0.0411, 0.0366, 0.0406,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0432, 0.0376, 0.0534,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          ...,\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543],\n",
            "          [0.0483, 0.0660, 0.0300,  ..., 0.0543, 0.0543, 0.0543]],\n",
            "\n",
            "         [[0.0393, 0.0791, 0.0315,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0448, 0.0678, 0.0431,  ..., 0.0393, 0.0393, 0.0393],\n",
            "          [0.0399, 0.0686, 0.0424,  ..., 0.0538, 0.0538, 0.0538],\n",
            "          ...,\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452],\n",
            "          [0.0489, 0.0553, 0.0480,  ..., 0.0452, 0.0452, 0.0452]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0789, 0.0462, 0.0710,  ..., 0.0191, 0.0191, 0.0191],\n",
            "          [0.0535, 0.0519, 0.0901,  ..., 0.0288, 0.0288, 0.0288],\n",
            "          [0.0454, 0.0511, 0.0363,  ..., 0.0439, 0.0439, 0.0439],\n",
            "          ...,\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567],\n",
            "          [0.0301, 0.0348, 0.0425,  ..., 0.0567, 0.0567, 0.0567]],\n",
            "\n",
            "         [[0.0702, 0.0637, 0.0403,  ..., 0.0496, 0.0496, 0.0496],\n",
            "          [0.0769, 0.0389, 0.0429,  ..., 0.0491, 0.0491, 0.0491],\n",
            "          [0.0272, 0.0450, 0.0356,  ..., 0.0747, 0.0747, 0.0747],\n",
            "          ...,\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645],\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645],\n",
            "          [0.0270, 0.0415, 0.0509,  ..., 0.0645, 0.0645, 0.0645]],\n",
            "\n",
            "         [[0.0874, 0.0660, 0.0550,  ..., 0.0404, 0.0404, 0.0404],\n",
            "          [0.0483, 0.0596, 0.0646,  ..., 0.0261, 0.0261, 0.0261],\n",
            "          [0.0557, 0.0307, 0.0536,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          ...,\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0349, 0.0474, 0.0672,  ..., 0.0434, 0.0434, 0.0434]]],\n",
            "\n",
            "\n",
            "        [[[0.0335, 0.0415, 0.0440,  ..., 0.0706, 0.0706, 0.0706],\n",
            "          [0.0441, 0.0396, 0.0489,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0412, 0.0673, 0.0596,  ..., 0.0518, 0.0518, 0.0518],\n",
            "          ...,\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577],\n",
            "          [0.0445, 0.0248, 0.0651,  ..., 0.0577, 0.0577, 0.0577]],\n",
            "\n",
            "         [[0.0560, 0.0376, 0.0757,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0394, 0.0360, 0.0400,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          [0.0503, 0.0340, 0.0552,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          ...,\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564],\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564],\n",
            "          [0.0848, 0.0686, 0.0462,  ..., 0.0564, 0.0564, 0.0564]],\n",
            "\n",
            "         [[0.0327, 0.0391, 0.0349,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0459, 0.0684, 0.0274,  ..., 0.0397, 0.0397, 0.0397],\n",
            "          [0.0461, 0.0740, 0.0627,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          ...,\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0632, 0.0541, 0.0637,  ..., 0.0443, 0.0443, 0.0443]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0367, 0.0556, 0.0599,  ..., 0.0392, 0.0392, 0.0392],\n",
            "          [0.0727, 0.0527, 0.0678,  ..., 0.0293, 0.0293, 0.0293],\n",
            "          [0.0528, 0.0507, 0.0694,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          ...,\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571],\n",
            "          [0.0404, 0.0351, 0.0405,  ..., 0.0571, 0.0571, 0.0571]],\n",
            "\n",
            "         [[0.0409, 0.0640, 0.0423,  ..., 0.0587, 0.0587, 0.0587],\n",
            "          [0.0420, 0.0381, 0.0413,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0427, 0.0466, 0.0632,  ..., 0.0423, 0.0423, 0.0423],\n",
            "          ...,\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0398, 0.0386, 0.0747,  ..., 0.0600, 0.0600, 0.0600]],\n",
            "\n",
            "         [[0.0606, 0.0541, 0.0623,  ..., 0.0354, 0.0354, 0.0354],\n",
            "          [0.0808, 0.0584, 0.0598,  ..., 0.0255, 0.0255, 0.0255],\n",
            "          [0.0439, 0.0522, 0.0787,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          ...,\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0223, 0.0446, 0.0795,  ..., 0.0408, 0.0408, 0.0408]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
        "\n",
        "print(attn_dists)\n",
        "print(attn_dists.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy6D23tZ7icl"
      },
      "source": [
        "```python\n",
        "ì´í›„ ê³„ì‚°ëœ attention ê°’ì„ vê³¼ ê³±í•˜ì—¬ ìµœì¢… ê²°ê´ê°’ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-OX-ddDQ7icl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515421f9-d2c0-465d-b950-b372aad2900e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ]
        }
      ],
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGC1aj667icl"
      },
      "source": [
        "### ê° headì˜ ê²°ê³¼ ë³‘í•©(concat)\n",
        "```python\n",
        "ê° headì˜ ê²°ê³¼ë¬¼ì„ concatí•˜ê³  ë™ì¼ ì°¨ì›(d_model)ìœ¼ë¡œ linear transformation í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì—¬ê¸°ì„œ 'd_model' ì°¨ì›ìœ¼ë¡œ linear transformation í•˜ëŠ” ì´ìœ ëŠ” transformer ëª¨ë¸ì—ì„œ ì›ë˜ì˜ ë°ì´í„°ì™€ ë”í•˜ëŠ” ì—°ì‚°(residual connection)ì´ ì¡´ì¬í•˜ì—¬ ì´ë•Œ ì°¨ì›ì„ í†µì¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "residual connection ì—°ì‚°ì€ ì•„ë˜ ì´ë¯¸ì§€ì—ì„œ Self-Attention ë¸”ë¡ ì´í›„ Addì— í•´ë‹¹í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
        "\n",
        "residual connectionì€ ì•ì„  ê°•ì˜ì—ì„œ ë°°ìš´ resnetì—ì„œ ì†Œê°œëœ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
        "```\n",
        "![residual](https://github.com/Pjunn/GDSC_mlstudy/blob/main/7%EC%A3%BC%EC%B0%A8/transformer_resideual_layer_norm.png?raw=true)\n",
        "ì´ë¯¸ì§€ ì¶œì²˜: https://jalammar.github.io/illustrated-transformer/ <br><br>\n",
        "-[What is Residual Connection?](https://paperswithcode.com/method/residual-connection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TDcwSJXq7icv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d3d062-be69-4ef7-fbfc-542af5365508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
        "\n",
        "print(attn_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R2YMaNuT7icv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62782e8-81e6-42e7-a101-6bc0de1aa367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.0647e-01,  1.7614e-02,  1.5888e-02,  ..., -7.6811e-02,\n",
            "           4.7090e-02,  5.9217e-02],\n",
            "         [ 1.8358e-01,  6.6134e-02,  4.2202e-02,  ..., -7.3667e-02,\n",
            "           7.7152e-02,  7.9882e-02],\n",
            "         [ 2.3217e-01,  7.2016e-02,  2.5100e-02,  ..., -1.0642e-01,\n",
            "           6.3860e-02,  7.2267e-02],\n",
            "         ...,\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02],\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02],\n",
            "         [ 2.3534e-01,  5.0881e-02,  2.1790e-02,  ..., -7.7324e-02,\n",
            "           5.9469e-02,  9.7691e-02]],\n",
            "\n",
            "        [[ 3.5702e-01, -1.7859e-01,  3.9717e-02,  ...,  2.8024e-02,\n",
            "          -3.8113e-02,  8.8643e-02],\n",
            "         [ 3.9739e-01, -1.5972e-01,  7.6489e-02,  ...,  5.3090e-02,\n",
            "          -6.2734e-06,  2.7271e-02],\n",
            "         [ 4.0024e-01, -1.7384e-01,  5.6186e-02,  ...,  7.0403e-02,\n",
            "          -8.3607e-03,  3.8790e-02],\n",
            "         ...,\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02],\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02],\n",
            "         [ 3.6427e-01, -1.2832e-01,  6.8669e-02,  ...,  7.6367e-02,\n",
            "          -4.9820e-02,  7.2537e-02]],\n",
            "\n",
            "        [[ 1.8888e-01, -1.0315e-01,  4.6053e-02,  ...,  2.9630e-02,\n",
            "          -3.4317e-02, -3.9398e-02],\n",
            "         [ 2.3809e-01, -1.7388e-01,  4.3160e-03,  ..., -5.1572e-02,\n",
            "          -1.9559e-02, -7.7400e-02],\n",
            "         [ 2.1433e-01, -1.3635e-01,  7.1454e-03,  ..., -3.7201e-02,\n",
            "          -3.4568e-02, -1.1389e-01],\n",
            "         ...,\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02],\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02],\n",
            "         [ 2.3760e-01, -1.4974e-01, -1.7905e-02,  ..., -4.2604e-03,\n",
            "          -4.4291e-02, -5.0378e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.1422e-01, -1.5383e-01,  1.4641e-02,  ..., -1.6093e-01,\n",
            "          -8.8677e-03, -9.0119e-02],\n",
            "         [-1.2518e-01, -1.3702e-01,  5.0213e-02,  ..., -1.6496e-01,\n",
            "           3.1292e-02, -8.7259e-02],\n",
            "         [-1.0011e-01, -1.3629e-01,  5.5492e-02,  ..., -1.1238e-01,\n",
            "          -2.9258e-03, -7.9936e-02],\n",
            "         ...,\n",
            "         [-1.2910e-01, -1.3411e-01,  1.2819e-03,  ..., -1.6582e-01,\n",
            "           4.9708e-02, -9.1370e-02],\n",
            "         [-1.1669e-01, -1.5108e-01,  5.2810e-02,  ..., -1.6697e-01,\n",
            "           7.2773e-02, -7.3678e-02],\n",
            "         [-1.3154e-01, -1.4187e-01,  1.2646e-02,  ..., -1.4920e-01,\n",
            "           3.7681e-03, -1.0389e-01]],\n",
            "\n",
            "        [[ 8.0946e-02, -9.6081e-03,  4.2926e-03,  ..., -1.3398e-01,\n",
            "          -4.1658e-02, -3.3607e-02],\n",
            "         [ 6.0193e-02, -9.4748e-04, -3.4700e-02,  ..., -1.6770e-01,\n",
            "          -2.4062e-02, -5.0679e-02],\n",
            "         [ 6.7370e-02,  4.0241e-02, -1.1831e-02,  ..., -1.5772e-01,\n",
            "          -2.6232e-02, -4.7158e-02],\n",
            "         ...,\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02],\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02],\n",
            "         [ 1.2180e-01, -1.4888e-03, -4.9793e-02,  ..., -1.1690e-01,\n",
            "          -5.2194e-02, -2.4892e-02]],\n",
            "\n",
            "        [[ 1.5702e-01, -1.3273e-01, -1.5563e-01,  ..., -3.5365e-02,\n",
            "           4.6784e-02,  3.2669e-02],\n",
            "         [ 1.2019e-01, -1.2637e-01, -1.8690e-01,  ..., -9.1197e-02,\n",
            "           4.7907e-03,  7.1274e-02],\n",
            "         [ 1.2186e-01, -8.5784e-02, -1.6949e-01,  ..., -2.0013e-02,\n",
            "           2.7023e-02,  6.2415e-02],\n",
            "         ...,\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02],\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02],\n",
            "         [ 1.9093e-01, -1.0125e-01, -1.5082e-01,  ..., -9.4689e-04,\n",
            "           2.9360e-02,  6.9759e-02]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "outputs = w_0(attn_values)\n",
        "\n",
        "print(outputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OzymRkf7icv"
      },
      "source": [
        "#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë”© ]</b></font> ìœ„ì˜ ê³¼ì •ì„ ëª¨ë‘ í•©ì³ í•˜ë‚˜ì˜ Multi-head attention ëª¨ë“ˆì„ êµ¬í˜„í•´ ë´…ì‹œë‹¤.\n",
        "```python\n",
        "ğŸ™\n",
        "ì•„ë˜ì˜ Multi-head attention ëª¨ë“ˆì—ì„œ '#TODO'ë¥¼ ì±„ì›Œ ëª¨ë“ˆì„ ì™„ì„± ì‹œì¼œì£¼ì„¸ìš”.\n",
        "ìœ„ ì‹¤ìŠµì—ì„œ ë°°ìš´ ë‚´ìš©ì´ í° íŒíŠ¸ê°€ ë  ê±°ì˜ˆìš”!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gWcXvPm37icw"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, dim_model, num_heads):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "    assert dim_model % num_heads == 0\n",
        "\n",
        "    self.dim_model = dim_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = dim_model // num_heads\n",
        "\n",
        "    self.w_q = nn.Linear(dim_model, dim_model)\n",
        "    self.w_k = nn.Linear(dim_model, dim_model)\n",
        "    self.w_v = nn.Linear(dim_model, dim_model)\n",
        "    self.w_0 = nn.Linear(dim_model, dim_model)\n",
        "\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "    q = self.w_q(query)\n",
        "    k = self.w_k(key)\n",
        "    v = self.w_v(value)\n",
        "\n",
        "    attn_values = self.self_attention(q,k,v)\n",
        "\n",
        "    outputs = attn_values.view(-1, query.size(1), query.size(2))\n",
        "    outputs = self.w_0(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def self_attention(self,q,k,v):\n",
        "      scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k) # (B,n_head,L,L)\n",
        "      attn_dists = F.softmax(scores,dim=-1)\n",
        "      output = torch.matmul(attn_dists, v).transpose(1, 2).contiguous().view(q.shape[0], -1, d_model)\n",
        "\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Lvl4um7icw"
      },
      "outputs": [],
      "source": [
        "dim_model = 512\n",
        "num_heads = 8\n",
        "multihead_attn = MultiheadAttention(dim_model, num_heads)\n",
        "\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zd6I8vxD7icw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91781751-27b3-4065-9c5e-d038e9f0e403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ğŸ‰ğŸ‰ ì„±ê³µ!!! ğŸ‰ğŸ‰ğŸ‰\n"
          ]
        }
      ],
      "source": [
        "# ì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì‹¤ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤!\n",
        "if outputs.shape == batch_emb.shape:\n",
        "    print(\"ğŸ‰ğŸ‰ğŸ‰ ì„±ê³µ!!! ğŸ‰ğŸ‰ğŸ‰\")\n",
        "else:\n",
        "    print(\"ğŸ™ ë‹¤ì‹œ ë„ì „í•´ë´ìš”!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}